#!/usr/bin/env python3
"""
æµ‹è¯•AIRA_MoEå±‚é‡è¦æ€§è®¡ç®—ä¿®å¤
"""

import os
import sys
import torch

# è®¾ç½®ç¯å¢ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

def test_layer_importance_fix():
    """æµ‹è¯•å±‚é‡è¦æ€§è®¡ç®—ä¿®å¤"""
    print("=== æµ‹è¯•AIRA_MoEå±‚é‡è¦æ€§è®¡ç®—ä¿®å¤ ===")
    
    try:
        sys.path.insert(0, '/data1/ldz/CoLA/LLaMA-Factory/src')
        from llamafactory.hparams import ModelArguments, FinetuningArguments
        from llamafactory.model import load_model, load_tokenizer
        from llamafactory.train.sft.trainer import CustomSeq2SeqTrainer
        from torch.utils.data import DataLoader, TensorDataset
        
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹å‚æ•°
        model_args = ModelArguments(
            model_name_or_path="/data/Llama-3.1-8B-Instruct",
            cache_dir=None,
        )
        
        # åˆ›å»ºAIRA_MoEå¾®è°ƒå‚æ•° - å¯ç”¨å±‚çº§rankåˆ†é…
        finetuning_args = FinetuningArguments(
            finetuning_type="aira_moe",
            lora_rank=4,
            lora_alpha=8,
            lora_target=["q_proj", "v_proj"],
            num_A=1,
            num_B=1,
            use_layer_wise_rank=True,  # å¯ç”¨å±‚çº§rankåˆ†é…
            rank_budget=32,
            theta_type="lod",
            objective_function="log",
            use_awsvd_init=False,
            use_activation_aware=True,
            activation_aware_mode="inps",
            activation_normalize=True,
        )
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        tokenizer_module = load_tokenizer(model_args)
        tokenizer = tokenizer_module["tokenizer"]
        
        # è®¾ç½®padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = load_model(tokenizer, model_args, finetuning_args, is_trainable=True)
        
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"  - æ¨¡å‹ç±»å‹: {type(model)}")
        print(f"  - PEFTç±»å‹: {list(model.peft_config.values())[0].peft_type}")
        
        # åˆ›å»ºç®€å•çš„è®­ç»ƒæ•°æ® - åªä½¿ç”¨ä¸¤æ¡æµ‹è¯•æ•°æ®
        print("åˆ›å»ºæµ‹è¯•æ•°æ®...")
        sample_texts = [
            "Hello, this is a test for AIRA_MoE layer importance.",
            "The quick brown fox jumps over the lazy dog.",
        ]
        
        # åˆ›å»ºæ•°æ®é›† - ç¡®ä¿æ‰€æœ‰æ ·æœ¬é•¿åº¦ä¸€è‡´
        max_length = 32
        
        # tokenizeæ‰€æœ‰æ–‡æœ¬
        all_inputs = tokenizer(sample_texts, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
        
        # åˆ›å»ºDataLoader
        class SimpleDataset(torch.utils.data.Dataset):
            def __init__(self, input_ids, attention_mask):
                self.input_ids = input_ids
                self.attention_mask = attention_mask
            
            def __len__(self):
                return len(self.input_ids)
            
            def __getitem__(self, idx):
                return {
                    'input_ids': self.input_ids[idx],
                    'attention_mask': self.attention_mask[idx]
                }
        
        dataset = SimpleDataset(all_inputs['input_ids'], all_inputs['attention_mask'])
        dataloader = DataLoader(dataset, batch_size=2, shuffle=False)  # ä½¿ç”¨batch_size=2
        
        print("âœ“ æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
        print(f"  - æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"  - Batchå¤§å°: 2")
        
        # æµ‹è¯•å±‚é‡è¦æ€§è®¡ç®—
        print("æµ‹è¯•å±‚é‡è¦æ€§è®¡ç®—...")
        
        # ç›´æ¥è°ƒç”¨compute_layer_importanceæ–¹æ³•
        device = str(next(model.parameters()).device)
        layer_importance = model.compute_layer_importance(
            dataloader,
            device=device,
            max_samples=2  # åªä½¿ç”¨2ä¸ªæ ·æœ¬
        )
        
        print("âœ“ å±‚é‡è¦æ€§è®¡ç®—æˆåŠŸ")
        print(f"  - è®¡ç®—äº† {len(layer_importance)} ä¸ªå±‚çš„é‡è¦æ€§")
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
        for i, (layer_name, importance) in enumerate(layer_importance.items()):
            if i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  - {layer_name}: LOD={importance['lod_mean']:.4f}, Act={importance['activation_mean']:.4f}")
        
        # æµ‹è¯•rankä¼˜åŒ–
        print("æµ‹è¯•rankä¼˜åŒ–...")
        rank_allocation = model.optimize_layer_ranks(
            layer_importance, 
            objective_function="log"
        )
        
        print("âœ“ Rankä¼˜åŒ–æˆåŠŸ")
        print(f"  - ä¼˜åŒ–äº† {len(rank_allocation)} ä¸ªå±‚çš„rank")
        
        total_ranks = sum(rank_allocation.values())
        print(f"  - æ€»rankä½¿ç”¨: {total_ranks}/{finetuning_args.rank_budget}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæµ‹è¯•"""
    print("AIRA_MoE å±‚é‡è¦æ€§è®¡ç®—ä¿®å¤æµ‹è¯•")
    print("=" * 50)
    
    success = test_layer_importance_fix()
    
    print("\n" + "=" * 50)
    if success:
        print("ğŸ‰ æµ‹è¯•é€šè¿‡ï¼å±‚é‡è¦æ€§è®¡ç®—ä¿®å¤æˆåŠŸï¼")
        print("\nç°åœ¨å¯ä»¥å®‰å…¨ä½¿ç”¨generality_full.shè¿›è¡Œå®Œæ•´çš„AIRA_MoEè®­ç»ƒ")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")

if __name__ == "__main__":
    main() 