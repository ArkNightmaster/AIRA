#!/usr/bin/env python3
"""
AIRA_MoE Fixed Integration Test
éªŒè¯ä¿®å¤åçš„AIRA_MoEåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import torch

# è®¾ç½®ç¯å¢ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("=== AIRA_MoE åŸºæœ¬åŠŸèƒ½æµ‹è¯• ===")
    
    try:
        # æµ‹è¯•PEFTåº“ä¸­çš„AIRA_MoEå¯¼å…¥
        from peft import AiraMoeConfig, AiraMoeModel, get_peft_model, TaskType
        print("âœ“ PEFTåº“ä¸­AIRA_MoEå¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•LLaMA-Factoryä¸­çš„å‚æ•°å¤„ç†
        sys.path.insert(0, '/data1/ldz/CoLA/LLaMA-Factory/src')
        from llamafactory.hparams import FinetuningArguments
        
        finetuning_args = FinetuningArguments(
            finetuning_type="aira_moe",
            lora_rank=8,
            lora_alpha=16,
            lora_target="all",
            num_A=2,
            num_B=2,
            # AwLoRAæŠ€æœ¯å‚æ•°
            use_layer_wise_rank=True,
            rank_budget=64,
            theta_type="lod",
            objective_function="log",
            use_awsvd_init=True,
            awsvd_collect_steps=50,
            use_activation_aware=True,
            activation_aware_mode="inps",
            activation_normalize=True,
        )
        
        print("âœ“ LLaMA-Factoryå‚æ•°å¤„ç†æˆåŠŸ")
        print(f"  - finetuning_type: {finetuning_args.finetuning_type}")
        print(f"  - use_layer_wise_rank: {finetuning_args.use_layer_wise_rank}")
        print(f"  - use_awsvd_init: {finetuning_args.use_awsvd_init}")
        print(f"  - use_activation_aware: {finetuning_args.use_activation_aware}")
        
        return True
        
    except Exception as e:
        print(f"âœ— åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("\n=== AIRA_MoE æ¨¡å‹åˆ›å»ºæµ‹è¯• ===")
    
    try:
        sys.path.insert(0, '/data1/ldz/CoLA/LLaMA-Factory/src')
        from llamafactory.hparams import ModelArguments, FinetuningArguments
        from llamafactory.model import load_model, load_tokenizer
        
        print("æ­£åœ¨åŠ è½½Llama-3.1-8B-Instructæ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹å‚æ•°
        model_args = ModelArguments(
            model_name_or_path="/data/Llama-3.1-8B-Instruct",
            cache_dir=None,
        )
        
        # åˆ›å»ºAIRA_MoEå¾®è°ƒå‚æ•°
        finetuning_args = FinetuningArguments(
            finetuning_type="aira_moe",
            lora_rank=4,
            lora_alpha=8,
            lora_target=["q_proj", "v_proj"],  # åªé€‰æ‹©éƒ¨åˆ†æ¨¡å—ä»¥åŠ å¿«æµ‹è¯•
            num_A=2,
            num_B=3,
            use_layer_wise_rank=True,  # æš‚æ—¶ç¦ç”¨ä»¥ç®€åŒ–æµ‹è¯•
            use_awsvd_init=False,       # æš‚æ—¶ç¦ç”¨ä»¥ç®€åŒ–æµ‹è¯•
            use_activation_aware=True,
            activation_aware_mode="inps",
            activation_normalize=True,
        )
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        tokenizer_module = load_tokenizer(model_args)
        tokenizer = tokenizer_module["tokenizer"]
        
        # è®¾ç½®padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = load_model(tokenizer, model_args, finetuning_args, is_trainable=True)
        
        print("âœ“ AIRA_MoEæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"  - æ¨¡å‹ç±»å‹: {type(model)}")
        print(f"  - æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
        
        # æ£€æŸ¥PEFTé…ç½®
        if hasattr(model, 'peft_config'):
            config = list(model.peft_config.values())[0]
            print(f"  - PEFTç±»å‹: {config.peft_type}")
            print(f"  - CoLAå‚æ•°: num_A={config.num_A}, num_B={config.num_B}")
            print(f"  - æ¿€æ´»æ„ŸçŸ¥: {config.use_activation_aware}")
            print(f"  - æ¿€æ´»æ¨¡å¼: {config.activation_aware_mode}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("æµ‹è¯•å‰å‘ä¼ æ’­...")
        sample_text = "Hello, this is a test for AIRA_MoE."
        inputs = tokenizer(sample_text, return_tensors="pt", padding=True, truncation=True, max_length=64)
        
        # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥è§¦å‘AwSVDå’Œæ¿€æ´»æ„ŸçŸ¥
        model.train()
        
        with torch.no_grad():
            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):
                outputs = model(**inputs)
        
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  - è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
        print(f"  - è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")
        
        # æµ‹è¯•AwLoRAæ ¸å¿ƒæ–¹æ³•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(model, 'compute_layer_importance'):
            print("âœ“ æ¨¡å‹åŒ…å«å±‚é‡è¦æ€§è®¡ç®—æ–¹æ³•")
        if hasattr(model, 'optimize_layer_ranks'):
            print("âœ“ æ¨¡å‹åŒ…å«rankä¼˜åŒ–æ–¹æ³•")
        if hasattr(model, 'apply_layer_wise_ranks'):
            print("âœ“ æ¨¡å‹åŒ…å«rankåº”ç”¨æ–¹æ³•")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_trainer_integration():
    """æµ‹è¯•è®­ç»ƒå™¨é›†æˆ"""
    print("\n=== AIRA_MoE è®­ç»ƒå™¨é›†æˆæµ‹è¯• ===")
    
    try:
        sys.path.insert(0, '/data1/ldz/CoLA/LLaMA-Factory/src')
        from llamafactory.train.sft.trainer import CustomSeq2SeqTrainer
        from llamafactory.hparams import FinetuningArguments
        
        # æ£€æŸ¥è®­ç»ƒå™¨æ˜¯å¦æœ‰AIRA_MoEæ”¯æŒæ–¹æ³•
        trainer_methods = dir(CustomSeq2SeqTrainer)
        
        if '_setup_aira_moe_layer_wise_ranks' in trainer_methods:
            print("âœ“ è®­ç»ƒå™¨åŒ…å«AIRA_MoEå±‚çº§rankåˆ†é…æ–¹æ³•")
        else:
            print("âœ— è®­ç»ƒå™¨ç¼ºå°‘AIRA_MoEå±‚çº§rankåˆ†é…æ–¹æ³•")
            return False
            
        if 'train' in trainer_methods:
            print("âœ“ è®­ç»ƒå™¨åŒ…å«é‡å†™çš„trainæ–¹æ³•")
        else:
            print("âœ— è®­ç»ƒå™¨ç¼ºå°‘é‡å†™çš„trainæ–¹æ³•")
            return False
        
        print("âœ“ è®­ç»ƒå™¨é›†æˆæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— è®­ç»ƒå™¨é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("AIRA_MoE ä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 50)
    
    # è¿è¡Œæµ‹è¯•
    test1_passed = test_basic_functionality()
    test2_passed = test_model_creation()
    test3_passed = test_trainer_integration()
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"âœ“ åŸºæœ¬åŠŸèƒ½æµ‹è¯•: {'é€šè¿‡' if test1_passed else 'å¤±è´¥'}")
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæµ‹è¯•: {'é€šè¿‡' if test2_passed else 'å¤±è´¥'}")
    print(f"âœ“ è®­ç»ƒå™¨é›†æˆæµ‹è¯•: {'é€šè¿‡' if test3_passed else 'å¤±è´¥'}")
    
    if all([test1_passed, test2_passed, test3_passed]):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AIRA_MoEå·²æˆåŠŸä¿®å¤å¹¶é›†æˆåˆ°LLaMA-Factoryä¸­ï¼")
        print("\nç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿›è¡Œè®­ç»ƒ:")
        print("  - ./scripts/aira_moe/generality_basic.sh")
        print("  - ./scripts/aira_moe/generality_standard.sh")
        print("  - ./scripts/aira_moe/generality_full.sh")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")

if __name__ == "__main__":
    main() 